{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-trained and transfer learning with keras\n",
    "\n",
    "Oftentimes, we are not going to reinvent the wheel, but use a **pre-trained network** to recognize entities. For some applications, there are very performant out-of-the-box solutions, and making your own model from scratch would be a waste of your valuable time.\n",
    "\n",
    "Other times, you have an application for which the out-of-the-box solutions are not exactly what you need. For this, you could use **transfer learning** to adapt a network for your use case. We will view both cases here.\n",
    "\n",
    "## 1. Pre-trained model usage\n",
    "\n",
    "In this chapter, we will learn how to import a **Keras** model that has been pre-trained to recognize various objects in full-color images.\n",
    "\n",
    "### 1.1 Importing an existing model\n",
    "\n",
    "Head on over to the official [Keras applications](https://keras.io/api/applications/) webpage to check out what models are there to pick and choose for our object recognition task.\n",
    "\n",
    "For this exercise, **you** can choose what network to pick. Look online what models are often used, or look at the evaluation metrics on the given webpage to determine which one is fit for the job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import your chosen model!\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "\n",
    "# Make a model object. Make sure you include the top as well!\n",
    "model = ResNet50(weights='imagenet')\n",
    "\n",
    "# Look at the model architecture.\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Preparing your images\n",
    "\n",
    "Go raid your Google photos or the World Wide Web for some pictures you want to get identified! Be sure to **pick pictures with only one clear object** inside. When you get to OpenCV model usage, you'll learn how to properly identify regions of interest in an image, but let's take one thing at a time here and keep our images simple.\n",
    "\n",
    "Keras also has a neat function to preprocess images **in the same way the training images for a network were preprocessed**. This is pretty handy to improve the model performance with those seedy pictures you found in your personal stash.\n",
    "\n",
    "Things like pixel scaling or color contrasting are applied using these functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the keras preprocessing method.\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "# Load your image. Make sure it is loaded in with the right dimensions for your model!\n",
    "image_size = (224, 224)\n",
    "original_image = image.load_img(\"/home/soukaina/project_crosswalk/Crosswalk-Detection/assets/rick.jpg\", target_size=image_size)\n",
    "\n",
    "# Convert your image pixels to a numpy array of values .\n",
    "image_array = image.img_to_array(original_image)\n",
    "\n",
    "# Reshape your image dimensions so that the colour channels correspond to what your model expects.\n",
    "image_array = image_array.reshape((1, image_array.shape[0], image_array.shape[1], image_array.shape[2]))\n",
    "\n",
    "# Preprocess your image with preprocess_input.\n",
    "prepared_image = preprocess_input(image_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Predicting the class of your image\n",
    "\n",
    "Let's take this bad boy for a spin! Can your image get properly identified? Finish the code down below and find out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.resnet50 import decode_predictions\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Predict the class of your picture.\n",
    "prediction = model.predict(prepared_image)\n",
    "\n",
    "# Decode your prediction into a set of human readable labels.\n",
    "labels = decode_predictions(prediction)\n",
    "\n",
    "# Get the most likely result from your set of labels.\n",
    "label = labels[0][0]\n",
    "\n",
    "# Print out your result.\n",
    "print(label)\n",
    "\n",
    "# Show handsome rick.\n",
    "plt.imshow(original_image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, how did we do? your final result should return a general label of the object in the picture, but the performance might not be up to your expectation. Why is this? \n",
    "\n",
    "This could be due to the fact that these are general object recognition models, they are trained on **a lot** of image, but also a lot of different classes of images. What would happen if you took the best part of your pre-trained network, and adapt the output to specialize to just a few of your own chosen classes? Let's check this out in the next part...\n",
    "\n",
    "## 2. Transfer learning\n",
    "\n",
    "Many of the steps of pre-trained model learning still apply to transfer learning, but this time around, we're going to use our own dataset to help the model specialize in a particular subject.\n",
    "\n",
    "Take a look at the image below: \n",
    "![low-to-high](./../assets/low-to-high.png)\n",
    "\n",
    "It shows how a particular object is recognized at the different stages of a CNN. With our pre-trained CNN, it's no different. Instead of retraining a full neural network to extract low-level features such as lines, or other common patterns, we **keep the general feature extraction** of a pre-trained network and **replace the nonlinear association of features** done in the last few dense layers of the network.\n",
    "\n",
    "Some features may be shared by multiple classes in our use case (with animals, this is often the case), while others are only specific to a single class of the network. That's quite okay though, even if the model has been trained to extract features from classes not present in the final set of classes, this is training you don't have to redo anyway. The only difference is a small redundancy and (non-noticeable) slowdown during model prediction because of the larger model size.\n",
    "\n",
    "So in short, the advantages of transfer learning are:\n",
    "- No need to train a whole model from scratch\n",
    "- less training data required (less DOF(degrees of freedom) to fill)\n",
    "- lower level feature extraction from pre-trained networks can be reused\n",
    "- output classes can be changed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Importing and preprocessing the data\n",
    "\n",
    "Having a general model is all fine and dandy, but I want to **relax** the object identification problem by **reducing** the amount of classes in our model. I want a model that can accurately **identify hot dogs** from not hot dogs!\n",
    "\n",
    "Go and download the data from [this Kaggle dataset](https://www.kaggle.com/dansbecker/hot-dog-not-hot-dog), and go through the motions of preparing the data for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def get_preprocessed_images(images_directory: str, image_size: tuple) -> list:\n",
    "    images = []\n",
    "    for img in os.listdir(images_directory):\n",
    "        img = image.load_img(images_directory+img, target_size=image_size)\n",
    "        img = image.img_to_array(img)\n",
    "        img = img.reshape((1, img.shape[0], img.shape[1], img.shape[2]))\n",
    "        img = preprocess_input(img)\n",
    "        images.append(img)\n",
    "    return np.vstack(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the keras preprocessing method.\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import pandas as pd\n",
    "\n",
    "# Load your images and preprocess them.\n",
    "cross_walk_images = get_preprocessed_images('/media/soukaina/60e39ea7-47c9-4d04-bb64-bfa6df74089f/adham/data/crosswalk/', image_size)\n",
    "cross_walk_labels = [\"cross_walk\"] * cross_walk_images.shape[0]\n",
    "\n",
    "non_cross_walk_images = get_preprocessed_images(\"/media/soukaina/60e39ea7-47c9-4d04-bb64-bfa6df74089f/adham/data/street/\", image_size)\n",
    "non_cross_walk_labels = [\"not_cross_walk\"] * non_cross_walk_images.shape[0]\n",
    "\n",
    "\n",
    "# Concatenate your images and your labels into X and y.\n",
    "X = np.concatenate([cross_walk_images, non_cross_walk_images])\n",
    "y = pd.get_dummies(cross_walk_labels + non_cross_walk_labels)\n",
    "\n",
    "print(X.shape, y.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As always when training a model, we need to split our data in a train and a test set. For deep learning, a validation set is oftentimes used as well. Kind of foggy on the [different purposes](https://towardsdatascience.com/train-validation-and-test-sets-72cb40cba9e7) of these sets?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
    "    X, \n",
    "    y,\n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train_val, \n",
    "    y_train_val,\n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Augmenting the data\n",
    "\n",
    "We don't have that many hot dogs to look at, let's squeeze as much information out of the training data by augmenting them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Determine the number of generated samples you want per original sample.\n",
    "datagen_batch_size = 16\n",
    "\n",
    "# Make a datagenerator object using ImageDataGenerator.\n",
    "train_datagen = ImageDataGenerator(rotation_range=60,\n",
    "                                    horizontal_flip=True)\n",
    "\n",
    "# Feed the generator your train data.\n",
    "train_generator = train_datagen.flow(X_train, y_train, batch_size=datagen_batch_size)\n",
    "\n",
    "# Make a datagenerator object using ImageDataGenerator.\n",
    "validation_datagen = ImageDataGenerator(rotation_range=60,\n",
    "                                        horizontal_flip=True)\n",
    "\n",
    "# Feed the generator your validation data.\n",
    "validation_generator = validation_datagen.flow(X_val, y_val, batch_size=datagen_batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Importing part of an existing model\n",
    "\n",
    "Let's import one of the application of the keras library, but make sure you don't import the final layers (the dense layers used to associate features). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import your chosen model!\n",
    "from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2\n",
    "\n",
    "# Make a model object. \n",
    "# Make sure you exclude the top part. set the input shape of the model to 224x224 pixels, with 3 color channels.\n",
    "model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(224, 224,3))\n",
    "\n",
    "# Freeze the imported layers so they cannot be retrained.\n",
    "for layer in model.layers:\n",
    "    layer.trainable = False\n",
    "    \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Adding flattening and dense layers\n",
    "\n",
    "Right now, our model is missing a top to actually classify our features. Let's add them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dropout\n",
    "\n",
    "new_model = Sequential()\n",
    "new_model.add(model)\n",
    "new_model.add(Flatten())\n",
    "new_model.add(Dense(64, activation='relu'))\n",
    "new_model.add(Dropout(0.5))\n",
    "new_model.add(Dense(2, activation='sigmoid'))\n",
    "\n",
    "# Summarize.\n",
    "new_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Training and evaluating the model\n",
    "\n",
    "All that's left to do is to train the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile and fit the model. Use the Adam optimizer and crossentropical loss. \n",
    "# Use the validation data argument during fitting to include your validation data.\n",
    "new_model.compile(optimizer='adam',\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "history = new_model.fit(train_generator,\n",
    "                        epochs=10, \n",
    "                        validation_data=validation_generator\n",
    "                       )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's see how you did with this handy helper function!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "\n",
    "def plot_history(history):\n",
    "    \"\"\" This helper function takes the tensorflow.python.keras.callbacks.History\n",
    "    that is output from your `fit` method to plot the loss and accuracy of\n",
    "    the training and validation set.\n",
    "    \"\"\"\n",
    "    fig, axs = plt.subplots(1,2, figsize=(12,6))\n",
    "    axs[0].plot(history.history['accuracy'], label='training set')\n",
    "    axs[0].plot(history.history['val_accuracy'], label = 'validation set')\n",
    "    axs[0].set(xlabel = 'Epoch', ylabel='Accuracy', ylim=[0, 1])\n",
    "\n",
    "    axs[1].plot(history.history['loss'], label='training set')\n",
    "    axs[1].plot(history.history['val_loss'], label = 'validation set')\n",
    "    axs[1].set(xlabel = 'Epoch', ylabel='Loss', ylim=[0, 10])\n",
    "    \n",
    "    axs[0].legend(loc='lower right')\n",
    "    axs[1].legend(loc='lower right')\n",
    "    \n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's evaluate on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First let's do some reverse one-hot encoding to get the labels back\n",
    "y_test_labels = y_test.idxmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's apply the model on X_test and get the label from the prediction\n",
    "y_pred = new_model.predict_classes(X_test)\n",
    "y_pred_labels = [\"cross_walk\" if pred == 0 else \"non_cross_walk\" for pred in y_pred]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test_labels, y_pred_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "ConfusionMatrixDisplay.from_predictions(y_test_labels, y_pred_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's test our model on new images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5wSOENK.jpg\n",
    "# img = image.load_img(\"../assets/hotdog.jpeg\", target_size=image_size)\n",
    "# img = image.load_img(\"/home/soukaina/Downloads/frikandel.jpg\", target_size=(224,224))\n",
    "\n",
    "# img = image.load_img(\"/media/soukaina/60e39ea7-47c9-4d04-bb64-bfa6df74089f/adham/pexels-tiana-614484.jpg\", target_size=(224, 224))\n",
    "img = image.load_img(\"/media/soukaina/60e39ea7-47c9-4d04-bb64-bfa6df74089f/adham/Zebra-in-field-e1653793342190.jpg\", target_size=(224, 224))\n",
    "\n",
    "img = image.img_to_array(img)\n",
    "img = img.reshape((1, img.shape[0], img.shape[1], img.shape[2]))\n",
    "img = preprocess_input(img)\n",
    "\n",
    "tag = new_model.predict_classes(img)\n",
    "\n",
    "if tag[0] == 0:\n",
    "    print(\"cross walk\")\n",
    "else:\n",
    "    print(\"Not cross walk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
